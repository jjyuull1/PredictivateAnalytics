# -*- coding: utf-8 -*-
"""Predictive_Analytics_JuliArsiSabrina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pERsy_Nnnd44mXx5mdw1Rv2PQ3Pbvg93

# **Predictive Analytics: Prediksi Diabetes**


---

Diabetes melitus adalah penyakit metabolik kronis yang ditandai oleh kadar gula darah tinggi akibat gangguan produksi atau fungsi insulin. Penyakit ini menjadi ancaman global dengan prevalensi yang terus meningkat. Data dari International Diabetes Federation (IDF) tahun 2021 menunjukkan bahwa lebih dari 537 juta orang di dunia hidup dengan diabetes, dan diperkirakan akan meningkat menjadi 643 juta pada tahun 2030. [**`diabetes`**](https://ejournal.itekes-bali.ac.id/jrkn/article/download/350/198)

Proyek ini menggunakan dataset **Pima Indians Diabetes** dari Kaggle untuk membangun model prediktif yang dapat mengidentifikasi individu dengan potensi risiko diabetes berdasarkan faktor seperti kadar glukosa, tekanan darah, indeks massa tubuh (BMI), dan usia. [`dataset`](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)

## **1. Business Understanding**
Membangun model klasifikasi berbasis machine learning yang dapat memprediksi status diabetes seorang pasien secara prediktif berdasarkan fitur-fitur medis

## **2. Data Understanding**
Proyek ini menggunakan dataset Pima Indians Diabetes dari Kaggle untuk membangun model prediktif yang dapat mengidentifikasi individu dengan potensi risiko diabetes berdasarkan faktor seperti kadar glukosa, tekanan darah, indeks massa tubuh (BMI), dan usia. [dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)

### **2.1 Data Loading**
Pada tahap ini, beberapa langkah dilakukan untuk memuat dataset yang akan digunakan dalam analisis.

#### **Import Library yang dibutuhkan**
Pada tahap awal proyek ini, dimulai dengan mengimpor berbagai library yang akan digunakan untuk analisis dan pemodelan data.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression

from google.colab import files
print("Silakan upload file kaggle.json")
files.upload()

#Membuat direktori .kaggle dan menyalin file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset dari Kaggle
!kaggle datasets download -d uciml/pima-indians-diabetes-database

import zipfile

with zipfile.ZipFile("pima-indians-diabetes-database.zip", "r") as zip_ref:
    zip_ref.extractall("dataset")

print("Dataset berhasil diunduh dan diekstrak ke folder 'dataset'")

"""### **2.2 Exploratory Data Analysis (EDA)**

#### **2.2.1 Deskripsi Variabel**
"""

df = pd.read_csv('dataset/diabetes.csv')
df.head()

"""Dataframe diatas memiliki 9 kolom yaitu:

- `Pragnancies`: Jumlah kehamilan yang pernah dialami individu.  
- `Glucose`: Konsentrasi glukosa plasma (mg/dL) yang diukur selama tes toleransi glukosa oral.  
- `BloodPressure`: Tekanan darah diastolik (mm Hg).  
- `Skinthickness`: Ketebalan lipatan kulit (mm) di bagian tricep.  
- `Insulin`: Kadar insulin serum 2 jam (mu U/ml).  
- `BMI`: Indeks Massa Tubuh (BMI).  
- `DiabetesPedigree`: Fungsi silsilah diabetes, yang mewakili kemungkinan terkena diabetes berdasarkan riwayat keluarga.  
- `Age`: Usia individu (tahun).  
- `Outcome`: Label biner yang menunjukkan apakah individu menderita diabetes (1) atau tidak (0).




"""

df.info()

"""- **Jumlah Entri dan Kolom:**
Dataset berisi 768 entri (baris) dan 9 kolom. Setiap kolom mewakili atribut atau fitur yang berbeda. Setiap kolom dalam dataset tidak ada nilai yang hilang.
- **Tipe Data:**
Tipe data yang digunakan adalah int64 untuk sebagian besar kolom, kecuali BMI dan DiabetesPedigreeFunction, yang memiliki tipe float64.
"""

df.describe()

"""Fungsi `describe()` menyajikan ringkasan statistik deskriptif untuk setiap kolom numerik dalam dataset. Informasi yang ditampilkan meliputi:

- Count: jumlah total data (sampel) pada kolom tersebut.
- Mean: nilai rata-rata dari seluruh data pada kolom.
- Std: standar deviasi, mengukur seberapa besar sebaran data dari rata-ratanya.
- Min: nilai terkecil dalam kolom.
- 25%: kuartil pertama, yaitu nilai di bawah 25% data berada.
- 50%: kuartil kedua atau median, yaitu titik tengah dari distribusi data.
- 75%: kuartil ketiga, artinya 75% data berada di bawah nilai ini.
- Max: nilai terbesar dalam kolom.

#### **2.2.2 Menangani Missing Value dan Outliers**

##### Menangani Missing Value
Sebelum melanjutkan ke tahap analisis dan pemodelan, penting untuk memastikan bahwa tidak terdapat duplikasi maupun nilai hilang (missing value) dalam dataset.
"""

# Melihat data duplikat
df.duplicated().sum()

# Melihat data yang hilang
df.isnull().sum()

df.shape

"""#### Menangani Outliner
Pada tahapan menanagi Outliner, akan dilakukan dengan teknik visualisasi data (boxplot). Kemudian, akan menangani outliers dengan teknik IQR method.
"""

from IPython.display import display  # Untuk menampilkan data dalam bentuk tabel di notebook

# Menentukan kolom numerik yang perlu diperiksa
columns_to_check = df.select_dtypes(include='number').columns.tolist()
if 'Outcome' in columns_to_check:
    columns_to_check.remove('Outcome')  # Menghapus kolom 'Outcome' karena ini adalah target

# Fungsi untuk mendeteksi outlier menggunakan IQR (Interquartile Range)
def detect_outliers_iqr(df, columns):
    outliers = {}
    for col in columns:
        Q1 = df[col].quantile(0.25)  # Kuartil pertama
        Q3 = df[col].quantile(0.75)  # Kuartil ketiga
        IQR = Q3 - Q1  # Menghitung IQR
        lower_bound = Q1 - 1.5 * IQR  # Batas bawah untuk outlier
        upper_bound = Q3 + 1.5 * IQR  # Batas atas untuk outlier
        outlier_rows = df[(df[col] < lower_bound) | (df[col] > upper_bound)]  # Menyaring data yang berada di luar batas
        outliers[col] = outlier_rows  # Menyimpan hasil deteksi outlier per kolom
    return outliers  # Mengembalikan hasil outlier

# Mendapatkan outlier untuk kolom numerik
outlier_dict = detect_outliers_iqr(df, columns_to_check)

# Menampilkan hasil deteksi outlier
for col, rows in outlier_dict.items():
    print(f"\nOutlier pada kolom {col}:")
    if not rows.empty:
        display(rows[columns_to_check])  # Menampilkan baris dengan outlier
    else:
        print("Tidak ada outlier")

# Membuat Bloxplot
plt.figure(figsize=(15, 6))
sns.boxplot(data=df[columns_to_check])
plt.title("Boxplot untuk Mendeteksi Outlier")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

# Menentukan kolom numerik (kecuali Outcome)
columns_to_check = df.select_dtypes(include='number').columns.tolist()
if 'Outcome' in columns_to_check:
    columns_to_check.remove('Outcome')

# Menghapus outlier menggunakan metode IQR
def remove_outliers_iqr(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

# Menghapus outlier secara iteratif
def remove_outliers_iteratively(df, columns, max_iter=10):
    for i in range(max_iter):
        df_new = remove_outliers_iqr(df.copy(), columns)
        if df_new.shape[0] == df.shape[0]:
            break
        df = df_new
    return df

# Terapkan fungsi
df = remove_outliers_iteratively(df.copy(), columns_to_check, max_iter=10)

# Plot hanya satu boxplot (tanpa subplot kosong)
plt.figure(figsize=(10, 6))
sns.boxplot(data=df[columns_to_check])
plt.title("Setelah Menghapus Outlier")
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

# Menampilkan jumlah outlier yang terdeteksi di setiap kolom
outlier_dict = detect_outliers_iqr(df, columns_to_check)
for col, rows in outlier_dict.items():
    print(f"Jumlah outlier pada kolom {col}: {len(rows)}")

print(f"Jumlah baris sesudah: {df.shape[0]}")

"""Dataset sekarang telah bersih dan memiliki 556 sampel dari dataset awal yaitu 768 sampel.

#### **2.2.3 Univariate Analysis**
"""

import matplotlib.pyplot as plt

# Hitung jumlah masing-masing nilai pada kolom Outcome
outcome_counts = df['Outcome'].value_counts()
labels = ['Tidak Diabetes (0)', 'Diabetes (1)']
colors = ['#66b3ff', '#ff9999']

# Custom function untuk menampilkan count dan persen
def func(pct, allvals):
    absolute = int(round(pct/100.*sum(allvals)))
    return f"{absolute} orang\n({pct:.1f}%)"

plt.figure(figsize=(7,7))
explode = (0.05, 0.05)  # sedikit melepaskan tiap irisan
plt.pie(
    outcome_counts,
    labels=labels,
    autopct=lambda pct: func(pct, outcome_counts),
    startangle=140,
    colors=colors,
    explode=explode,
    pctdistance=0.7,   # jarak persen dari pusat
    labeldistance=1.1  # jarak label dari pusat
)
plt.title('Distribusi Outcome')
plt.axis('equal')
plt.show()

df.hist(bins=50, figsize=(20,15))
plt.show()

"""Dari hasil analisis histogram di atas, dapat menyimpulkan beberapa hal penting yang akan berimplikasi pada tahap analisis dan pemodelan selanjutnya:
- Beberapa kolom, seperti Insulin dan SkinThickness, menunjukkan distribusi yang sangat miring atau banyak nilai 0.
- Beberapa variabel, seperti Glucose, BloodPressure, dan BMI, menunjukkan distribusi yang cukup normal, yang berarti mereka bisa digunakan langsung tanpa masalah besar.
- Kolom Outcome menunjukkan distribusi kelas yang tidak seimbang, yang akan mempengaruhi cara model melakukan prediksi dan mungkin memerlukan teknik penyeimbangan kelas.

### **2.2.4 Multivariate Analysis**
"""

sns.pairplot(df, diag_kind = 'kde')

"""Dari pairplot ini, mendapatkan pemahaman yang lebih baik mengenai hubungan antar fitur, terutama fitur-fitur yang berhubungan dengan Outcome. Fitur seperti `age`, `insulin`, dan `BMI` menunjukkan adanya pola yang cukup relevan dengan kondisi diabetes (outcome = 1)."""

# Menghitung matriks korelasi untuk fitur numerik
correlation_matrix = df.corr()
# Membuat visualisasi heatmap dari matriks korelasi
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Matriks Korelasi Fitur Numerik')
plt.show()

"""## **3. Data Preparation**

### **3.1 Split Data**
Pada tahap ini, dataset dibagi menjadi dua bagian yaitu data pelatihan (training data) dan data pengujian (test data). Pemisahan ini dilakukan agar model dapat dilatih pada data pelatihan dan diuji pada data yang terpisah untuk memastikan kemampuan generalisasi model terhadap data yang tidak terlihat sebelumnya
"""

X = df.drop(columns=['Outcome'])
y = df['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""`test_size=0.2`: 20% data digunakan sebagai data pengujian, sisanya 80% sebagai data pelatihan.

`random_state=42`: Menjamin hasil pembagian data yang konsisten (reproducible).
"""

# Melihat distribusi kelas
print("Distribusi kelas pada data pelatihan (y_train):")
print(y_train.value_counts())
print("Distribusi kelas pada data pengujian (y_test):")
print(y_test.value_counts())

"""Pembagian data dilakukan dengan proporsi yang seimbang antara kelas dalam data pelatihan dan pengujian. Ini penting untuk menghindari bias pada model, terutama jika ada ketimpangan kelas (class imbalance). Parameter `random_state` digunakan agar eksperimen dapat direproduksi.

### **3.2 Standardisasi**
"""

from sklearn.preprocessing import StandardScaler
# Inisialisasi objek scaler
scaler = StandardScaler()

# Melakukan fitting dan transformasi hanya pada fitur (X)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Membuat DataFrame Hasil Standardisasi
df_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
df_scaled['Outcome'] = y_train.reset_index(drop=True)

df_scaled.head()

"""## **4. Model Development**

##### Support Vector Machine (SVM)
Pada tahap ini, dilakukan pembangunan model klasifikasi menggunakan algoritma Support Vector Machine (SVM) dengan kernel linear. Model dilatih menggunakan data pelatihan, kemudian dievaluasi dengan data pengujian untuk mengukur performa klasifikasi.
"""

# Inisialisasi model SVM
model_svm_linear = SVC(kernel='linear', C=1, random_state=42)
model_svm_linear.fit(X_train, y_train)

y_pred_svm = model_svm_linear.predict(X_test)
svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)
svm_acc_score = accuracy_score(y_test, y_pred_svm)

# Menampilkan nilai akurasi model
print("Nilai Akurasi untuk model SVM dengan Kernel Linear: {:.2f}%".format(svm_acc_score * 100, '%\n'))
print("Classification Report untuk SVM dengan Kernel Linear:")
print(classification_report(y_test, y_pred_svm))

# Membuat confusion matrix dari hasil prediksi
svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(6, 4))
sns.heatmap(svm_conf_matrix, annot=True, fmt='d', cmap='YlGnBu', cbar=True)
plt.title('Confusion Matrix untuk SVM dengan Kernel Linear')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Dari confusion matrix, dapat disimpulkan bahwa:

- Kelas 0 (negatif) diprediksi dengan sangat baik (precision 85%, recall 93%).

- Kelas 1 (positif) kurang optimal (precision 71%, recall 52%).

##### Logistic Regression
Model Logistic Regression ini digunakan untuk mengklasifikasikan data ke dalam dua kelas target berdasarkan fitur yang telah diproses sebelumnya.
"""

# Inisialisasi model Logistic Regression
logreg = LogisticRegression()

# Melakukan pelatihan model menggunakan data yang sudah diskalakan
logreg.fit(X_train_scaled, y_train)

# Memprediksi hasil pada data uji
y_pred = logreg.predict(X_test_scaled)

# Evaluasi model
accuracy = accuracy_score(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Menampilkan hasil evaluasi
print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{class_report}")

# Membuat confusion matrix dari hasil prediksi
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu', cbar=True)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.show()

"""Hasil:
- Kelas 0 (negatif) diprediksi sangat baik (precision tinggi dan recall tinggi).

- Kelas 1 (positif) memiliki precision cukup baik, namun recall masih rendah.

## **5. Evaluasi Model**
Evaluasi dilakukan dengan membandingkan nilai akurasi dari masing-masing model berdasarkan data pengujian.
"""

# Menampilkan hasil akurasi
print("\nAkurasi Model:")
print(f"SVM (Kernel Linear): {svm_acc_score * 100:.2f}%")
print(f"Logistic Regression: {accuracy_score(y_test, y_pred) * 100:.2f}%")

# Visualiasi Perbandingan Akurasi
models = ['SVM', 'Logistic Regression']
accuracies = [svm_acc_score, accuracy_score(y_test, y_pred)]

plt.figure(figsize=(8, 6))
sns.barplot(x=models, y=accuracies, palette='viridis')
plt.title('Perbandingan Akurasi Model')
plt.ylabel('Akurasi')
plt.ylim(0, 1)
plt.show()

"""### Evaluasi Model: Perbandingan Akurasi SVM dan Logistic Regression

Grafik di atas menunjukkan perbandingan akurasi antara dua model klasifikasi yang digunakan, yaitu **SVM (Support Vector Machine) dengan kernel linear** dan **Logistic Regression**.

- **SVM (Kernel Linear)** memperoleh akurasi sebesar **82.14%**.
- **Logistic Regression** sedikit lebih unggul dengan akurasi sebesar **83.04%**.

### Interpretasi:
- Kedua model memiliki performa yang cukup baik dengan akurasi di atas 80%.
- Meskipun perbedaan akurasi antara keduanya tidak signifikan (sekitar 0.9%), **Logistic Regression menunjukkan hasil yang sedikit lebih baik dibandingkan SVM dalam konteks dataset ini**.
- Selain itu, **SVM** mungkin lebih cocok untuk kasus dengan data berdimensi tinggi, sedangkan **Logistic Regression** seringkali lebih mudah diinterpretasikan dan diimplementasikan.
- Jika interpretabilitas dan efisiensi komputasi lebih diutamakan, maka Logistic Regression bisa menjadi pilihan. Namun jika fokus pada margin klasifikasi yang optimal dan ada kemungkinan data berdimensi tinggi, SVM tetap merupakan alternatif yang kuat.

"""